import { AnalysisResult, AISettings, ModelOption, BenCaoHerb, MedicalRecord, TreatmentPlanEntry, MedicalKnowledgeChunk } from "../types";
import { DEFAULT_RETRY_DELAY, MAX_RETRIES, VECTOR_API_URL, VECTOR_API_KEY, DEFAULT_EMBEDDING_MODEL } from "../constants";

export interface OpenAIToolCall {
    id: string;
    type: 'function';
    function: { name: string; arguments: string; };
}
export type OpenAIContentPart = | { type: 'text'; text: string } | { type: 'image_url'; image_url: { url: string; detail?: 'auto' | 'low' | 'high' } };
export interface OpenAIMessage {
    role: 'system' | 'user' | 'assistant' | 'tool';
    content?: string | null | OpenAIContentPart[];
    tool_calls?: OpenAIToolCall[];
    tool_call_id?: string;
    name?: string; 
}

// ==========================================
// 1. System Prompt Definitions (Chinese)
// ==========================================

export const createEmptyMedicalRecord = (): MedicalRecord => ({
  knowledgeChunks: [],
  basicInfo: { name: '', gender: '', age: '', marital: '', occupation: '', season: '' },
  chiefComplaint: '',
  historyOfPresentIllness: '',
  pastHistory: '',
  allergies: '',
  currentSymptoms: {
    coldHeat: '', sweat: '', headBody: '', stoolsUrine: '', diet: '', sleep: '', emotion: '', gynecology: '', patientFeedback: '',
  },
  physicalExam: { tongue: '', pulse: '', general: '', bloodPressureReadings: [], },
  auxExams: { labResults: [], other: '', },
  diagnosis: { tcm: '', western: '', treatmentPlans: [] },
});

export const TCM_Clinical_Logic_Calculator_Prompt = `
# Role: ä¸­åŒ»æ•°æ®è§£æ„ä¸é€»è¾‘èŒƒå¼åˆ†æå¸ˆ
## æ ¸å¿ƒä»»åŠ¡
å°†ã€Šå‡è®¾æ€§æ–¹å‰‚ä¸ç—…å†æ–‡æœ¬ã€‹è§£æ„å¹¶å‘ˆç°ä¸ºä¸€ä»½**è§†è§‰åŒ–ã€ç»“æ„æ¸…æ™°**çš„ã€Šé€»è¾‘è§£ææŠ¥å‘Šã€‹ã€‚ä½ éœ€è¦å……åˆ†åˆ©ç”¨ HTML/CSS ç»“æ„æ¥å¢å¼ºå¯è¯»æ€§ï¼Œæ¨¡ä»¿é«˜ç«¯åŒ»ç–—æ•°æ®åˆ†æä»ªè¡¨çš„æ˜¾ç¤ºé£æ ¼ã€‚

## è§†è§‰æ¸²æŸ“è§„èŒƒ (CSS Design System)
ä½ å¿…é¡»ç›´æ¥è¾“å‡º HTML ä»£ç ï¼ˆåµŒå…¥åœ¨ Markdown ä¸­ï¼‰ï¼Œå¹¶ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹ CSS ç±»æ¥æ„å»ºç•Œé¢ã€‚ä¸è¦ä½¿ç”¨å†…è”æ ·å¼ï¼Œåªä½¿ç”¨ä»¥ä¸‹ç±»åï¼š

1. **å®¹å™¨ä¸æ’ç‰ˆ**:
   - æŠ¥å‘Šå®¹å™¨è‡ªåŠ¨åº”ç”¨åŸºç¡€æ ·å¼ï¼Œä½ åªéœ€å…³æ³¨å†…å®¹ç»“æ„ã€‚
   - æ ‡é¢˜: ä½¿ç”¨ Markdown \`##\` (H2) ä½œä¸ºä¸»è¦ç« èŠ‚ï¼Œå®ƒä»¬ä¼šè‡ªåŠ¨è·å¾—é’è‰²å·¦è¾¹æ¡†å’Œæ¸å˜èƒŒæ™¯ã€‚
   - å¼ºè°ƒ: ä½¿ç”¨ \`<span class="tcm-highlight">å†…å®¹</span>\` é«˜äº®å…³é”®æ–‡æœ¬ã€‚

2. **å¡ç‰‡å¸ƒå±€ (Card Layout)**:
   - å°†æ‰€æœ‰åˆ†ææ¨¡å—æ”¾å…¥å¡ç‰‡ä¸­ï¼š\`<div class="tcm-card">...</div>\`
   - å¡ç‰‡å†…æ ‡é¢˜ï¼š\`<div class="tcm-card-header">æ ‡é¢˜</div>\`
   - åŒæ å¸ƒå±€ï¼ˆå¦‚å·¦å³å¯¹æ¯”ï¼‰ï¼š\`<div class="tcm-grid-2">...</div>\`

3. **å½©è‰²æ ‡ç­¾ (Status Tags)** - *ç”¨äºå…«çº²ã€ç—…æœºã€é£é™©ç­‰çº§*:
   - \`<span class="tcm-tag tag-teal">æ°”è™š/å¹³å’Œ/è¡¨è¯</span>\` (é’è‰²ï¼šåæ­£å‘æˆ–ä¸­æ€§)
   - \`<span class="tcm-tag tag-orange">è¡€ç˜€/å®çƒ­/æ°”æ»</span>\` (æ©™è‰²ï¼šå®è¯æˆ–è­¦ç¤º)
   - \`<span class="tcm-tag tag-indigo">å¯’æ¹¿/é˜´è™š/é‡Œè¯</span>\` (é›è“ï¼šé˜´æ€§æˆ–æ·±å±‚)
   - \`<span class="tcm-tag tag-rose">é«˜é£é™©/ç¦å¿Œ</span>\` (ç«ç‘°çº¢ï¼šå±é™©)

4. **æç¤ºæ¡†**:
   - è­¦ç¤º: \`<div class="tcm-alert-box">...</div>\`
   - ä¿¡æ¯: \`<div class="tcm-info-box">...</div>\`

## æŠ¥å‘Šç»“æ„è“å›¾ (å¿…é¡»åŒ…å«ä»¥ä¸‹ç« èŠ‚)

### 01. è¾¨è¯åæ ‡ç³»æ„å»º (Dialectical Coordinates)
*ä½¿ç”¨å¡ç‰‡å¸ƒå±€ã€‚*
- **å…«çº²å®šä½**: ä½¿ç”¨ \`tag-teal\`/\`tag-indigo\` ç­‰æ ‡ç­¾æ˜ç¡®å¯’çƒ­è™šå®è¡¨é‡Œã€‚
- **è„è…‘å®šä½**: æ˜ç¡®ç—…ä½ã€‚
- **æ’ä»–æ€§åˆ†æ**: ç®€è¿°ä¸ºä½•æ’é™¤å…¶ä»–ç›¸ä¼¼è¯å‹ã€‚

### 02. æ ¸å¿ƒçŸ›ç›¾ä¸é…ä¼é€»è¾‘ (Core Logic)
*ä½¿ç”¨åŒæ å¡ç‰‡å¸ƒå±€ (\`tcm-grid-2\`)ã€‚*
- **å·¦æ : å‡é™æµ®æ²‰åšå¼ˆ**: åˆ†ææ°”æœºæµå‘ã€‚
- **å³æ : è¯ç‰©è§’è‰²å®¡è®¡**: å“ªäº›æ˜¯å›è¯ï¼ˆåŠ ç²—ï¼‰ï¼Œå“ªäº›æ˜¯ä½ä½¿ã€‚
- **é€æ˜åŒ–è®¡ç®—**: å±•ç¤ºåŠ¿èƒ½æƒé‡çš„å®šæ€§ä¼°ç®—ã€‚

### 03. é£é™©æ‰«æä¸åŠ¨æ€è¿½è¸ª (Risk Scanning)
*ä½¿ç”¨è­¦ç¤ºæ¡† (\`tcm-alert-box\`)ã€‚*
- **å…³é”®é£é™©ç‚¹**: æŒ‡å‡ºæ–¹ä¸­å¯èƒ½å¼•èµ·ä¸è‰¯ååº”çš„é…ä¼ã€‚
- **é•¿æœŸæœç”¨é¢„è­¦**: é’ˆå¯¹æ‚£è€…ä½“è´¨çš„é•¿æœŸå»ºè®®ã€‚

### 04. ç»“æ¡ˆå®šæ€§ (Conclusion)
*ä½¿ç”¨ä¿¡æ¯æ¡† (\`tcm-info-box\`)ã€‚*
- **ç»¼åˆè¯„çº§**: ç»™å‡ºé€»è¾‘ä¸¥è°¨æ€§è¯„çº§ã€‚
- **ååŒ»æ˜ å°„**: æ­¤æ–¹ç±»ä¼¼å¤ä»£å“ªä¸ªåæ–¹ï¼ˆå¦‚â€œéšå–»ä¸ºæ¡‚ææ±¤å˜æ–¹â€ï¼‰ã€‚

---
**ç¦æ­¢äº‹é¡¹**:
- ä¸¥ç¦è¾“å‡º \`<html>\`, \`<body>\` ç­‰æ ¹èŠ‚ç‚¹æ ‡ç­¾ã€‚
- ä¸¥ç¦è¾“å‡ºä»»ä½•å¯èƒ½å½±å“å…¨å±€å¸ƒå±€çš„ CSS (å¦‚ \`position: fixed\`)ã€‚
- ç¡®ä¿æ‰€æœ‰ \`<div>\` æ ‡ç­¾éƒ½æ­£ç¡®é—­åˆï¼Œé¿å…ç ´åå¤–éƒ¨å®¹å™¨ã€‚

**è¾“å‡ºç¤ºä¾‹**:
\`\`\`html
<div class="tcm-card">
  <div class="tcm-card-header">å…«çº²å®šä½ä¸æ’ä»–æ€§åˆ†æ</div>
  <p>
    å®šä½ï¼š<span class="tcm-tag tag-teal">æœ¬è™šæ ‡å®</span> <span class="tcm-tag tag-indigo">å¯’æ¹¿å†…è•´</span>
  </p>
  <p>è™½æœ‰â€œé˜²é£â€åœ¨åˆ—ï¼Œä½†æ‚£è€…å’³å—½å·²å‡ï¼Œä¸»è¦çŸ›ç›¾å·²ç”±è¡¨å…¥é‡Œ...</p>
</div>
\`\`\`

è¯·ç°åœ¨å¼€å§‹åˆ†æï¼Œç›´æ¥è¾“å‡ºæ¸²æŸ“åçš„ HTML å†…å®¹ã€‚
`;

export const DEFAULT_ANALYZE_SYSTEM_INSTRUCTION = TCM_Clinical_Logic_Calculator_Prompt;

export const QUICK_ANALYZE_SYSTEM_INSTRUCTION = `
# è§’è‰²ï¼šä¸­åŒ»å¤„æ–¹å®‰å…¨å®¡æ ¸å‘˜
# ä»»åŠ¡ï¼šå¿«é€Ÿæ£€æŸ¥å¤„æ–¹é’ˆå¯¹å½“å‰ç—…å†çš„å®‰å…¨æ€§ä¸åˆç†æ€§ã€‚
# è§†è§‰è¦æ±‚ï¼šä½¿ç”¨ <div class="tcm-alert-box"> åŒ…è£¹é£é™©æç¤ºï¼Œä½¿ç”¨ <span class="tcm-tag tag-teal"> æ ‡è®°å®‰å…¨é¡¹ã€‚
# è¾“å‡ºï¼šç®€ç»ƒçš„ HTML ç‰‡æ®µã€‚
`;

export const CHAT_SYSTEM_INSTRUCTION_BASE = `
# è§’è‰²ï¼šé«˜çº§ä¸­åŒ»ä¸´åºŠå†³ç­–æ”¯æŒåŠ©æ‰‹ (CDSS)
# æŒ‡ä»¤ï¼š
- ä½ æ­£åœ¨ååŠ©åŒ»ç”Ÿåˆ†æä¸­åŒ»å¤„æ–¹ã€‚
- å›ç­”æ—¶ï¼Œå¦‚æœæ¶‰åŠå…³é”®åŒ»å­¦åˆ¤æ–­ï¼Œè¯·ä½¿ç”¨ç¾è§‚çš„ HTML æ ‡ç­¾æ¥å¢å¼ºå¯è¯»æ€§ã€‚
- ä½¿ç”¨ <span class="tcm-tag tag-orange">å…³é”®æ¦‚å¿µ</span> é«˜äº®æœ¯è¯­ã€‚
- ä½¿ç”¨ <div class="tcm-info-box"> åŒ…è£¹å»ºè®®ã€‚
- å¦‚æœç”¨æˆ·è¦æ±‚ä¿®æ”¹è¯ææ•°æ®ï¼Œè¯·è°ƒç”¨å·¥å…· \`update_herb_database\`ã€‚
- **æ ¼å¼**ï¼šæ··åˆ Markdown å’Œ HTML (ä½¿ç”¨ tcm-card, tcm-tag ç­‰ç±»å)ã€‚
- **è¯­è¨€**ï¼šç®€ä½“ä¸­æ–‡ã€‚
`;

export const MEDICAL_SEMANTIC_CHUNKING_PROMPT = `
# è§’è‰²ï¼šåŒ»ç–—çŸ¥è¯†è¯­ä¹‰èšåˆå¼•æ“ (Semantic Chunker)
# ä»»åŠ¡ï¼šå°†é›¶æ•£çš„åŒ»ç–—æ–‡æœ¬ï¼ˆåŒ…æ‹¬OCRæ‰«æä»¶ã€ç—…å†è®°å½•ï¼‰é‡ç»„ä¸ºå®Œæ•´çš„è¯­ä¹‰çŸ¥è¯†å—ã€‚

# æ ¸å¿ƒè§„åˆ™ (CRITICAL):
1. **ç¦æ­¢ç¢ç‰‡åŒ–**ï¼šä¸¥ç¦å°†ä¸€å¥è¯ã€ä¸€ä¸ªè¯Šæ–­ç»“è®ºæˆ–ä¸€é¡¹æ£€æŸ¥çš„å®Œæ•´æè¿°æ‹†åˆ†æˆå¤šä¸ªç‰‡æ®µã€‚å¦‚æœåŸæ–‡ä¸­å› ä¸ºæ¢è¡Œç¬¦å¯¼è‡´å¥å­æ–­è£‚ï¼Œ**å¿…é¡»**å°†å®ƒä»¬åˆå¹¶ã€‚
2. **å®Œæ•´è¯­ä¹‰**ï¼šæ¯ä¸ªçŸ¥è¯†å—å¿…é¡»æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ã€è¯­ä¹‰å®Œæ•´çš„é™ˆè¿°ã€‚
   - é”™è¯¯ç¤ºä¾‹ï¼šChunk1: "OM2", Chunk2: "å±äº...", Chunk3: "éé˜»å¡æ€§..."
   - æ­£ç¡®ç¤ºä¾‹ï¼šChunk1: "OM2 (ç¬¬äºŒé’ç¼˜æ”¯) å±äºä¸­å±‚éé˜»å¡æ€§å† å¿ƒç—…ï¼Œç®¡è…”ä¸­åº¦ç‹­çª„ã€‚"
3. **æ ‡ç­¾åˆ†ç±»**ï¼šå‡†ç¡®è¯†åˆ«å†…å®¹å¹¶æ‰“ä¸Šæ ‡ç­¾ï¼ˆå¦‚ï¼šä¸»è¯‰ã€ç°ç—…å²ã€è¶…å£°å¿ƒåŠ¨å›¾ã€å† è„‰é€ å½±ã€è¥¿åŒ»è¯Šæ–­ã€ä¸­åŒ»è¯Šæ–­ã€ç”¨è¯è®°å½•ï¼‰ã€‚
4. **æ•°å€¼ä¿ç•™**ï¼šæ‰€æœ‰çš„æ£€æµ‹æ•°å€¼ã€æ—¥æœŸå¿…é¡»ä¿ç•™åœ¨ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œä¸å¯å•ç‹¬æˆå—ã€‚

# ç¤ºä¾‹è¾“å…¥ï¼š
"2025.11.09
å† çŠ¶åŠ¨è„‰
CTAæ˜¾ç¤ºï¼šå‰é™æ”¯
è¿‘æ®µæ··åˆæ–‘å—ï¼Œç®¡è…”
ä¸­åº¦ç‹­çª„(50-60%)ã€‚"

# ç¤ºä¾‹è¾“å‡ºï¼š
[
  { "content": "2025.11.09 å† çŠ¶åŠ¨è„‰CTAæ˜¾ç¤ºï¼šå‰é™æ”¯è¿‘æ®µæ··åˆæ–‘å—ï¼Œç®¡è…”ä¸­åº¦ç‹­çª„(50-60%)ã€‚", "tags": ["è¾…åŠ©æ£€æŸ¥", "CTA", "å¿ƒè¡€ç®¡"] }
]

# è¾“å‡ºæ ¼å¼ï¼š
çº¯ JSON æ•°ç»„ï¼Œä¸åŒ…å« markdown ä»£ç å—æ ‡è®°ã€‚
`;

export const MEDICAL_ORGANIZE_PROMPT = `
# è§’è‰²ï¼šåŒ»ç–—æ•°æ®ç»“æ„åŒ–å½’çº³å¼•æ“
# ä»»åŠ¡ï¼šæ•´ç†é›¶æ•£çš„ç—…å†ç‰‡æ®µï¼Œç”Ÿæˆç»“æ„åŒ–çš„æ±‡æ€»ä¿¡æ¯ã€‚é‡ç‚¹å…³æ³¨æ—¶é—´çº¿å’Œæ£€æŸ¥æ•°æ®ã€‚

# è¾“å…¥ï¼šä¸€ç³»åˆ—ç—…å†æ–‡æœ¬ç‰‡æ®µã€‚

# è¾“å‡ºè¦æ±‚ï¼š
è¯·ç”Ÿæˆä¸€ä¸ª Markdown æ ¼å¼çš„æ±‡æ€»æŠ¥å‘Šï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¦‚æœè¾“å…¥ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼‰ï¼š

1. **ç”Ÿå‘½ä½“å¾è¶‹åŠ¿**ï¼š
   - å°†æ‰€æœ‰è¡€å‹ (BP)ã€å¿ƒç‡ (HR) æ•°æ®æŒ‰æ—¶é—´é¡ºåºæ•´ç†æˆ Markdown è¡¨æ ¼ã€‚
   - è¡¨å¤´ï¼šæ—¥æœŸ | æ—¶é—´ | è¡€å‹ (mmHg) | å¿ƒç‡ (bpm) | å¤‡æ³¨ (ä½“ä½/çŠ¶æ€)
   - å¿…é¡»æŒ‰å¹´/æœˆ/æ—¥æ’åºã€‚

2. **å®éªŒå®¤æ£€æŸ¥æ±‡æ€»**ï¼š
   - å°†åŒä¸€ç±»å‹çš„æ£€æŸ¥ï¼ˆå¦‚è¡€å¸¸è§„ã€ç”ŸåŒ–ã€å‡è¡€ï¼‰å½’çº³åœ¨ä¸€èµ·ã€‚
   - ä½¿ç”¨è¡¨æ ¼å±•ç¤ºå…³é”®å¼‚å¸¸æŒ‡æ ‡åŠå…¶å˜åŒ–ã€‚
   - è¡¨å¤´ï¼šæ—¥æœŸ | æ£€æŸ¥é¡¹ç›® | å…³é”®æŒ‡æ ‡ | ç»“æœ | å‚è€ƒèŒƒå›´

3. **å…³é”®ç—…å²æ—¶é—´è½´**ï¼š
   - ç”¨åˆ—è¡¨å½¢å¼ç®€è¿°å‘ç—…ã€å°±è¯Šã€æ²»ç–—çš„å…³é”®èŠ‚ç‚¹ã€‚

# æ ¼å¼ç¤ºä¾‹ï¼š
## ğŸ©¸ è¡€å‹/å¿ƒç‡ç›‘æµ‹è®°å½•
| æ—¥æœŸ | æ—¶é—´ | è¡€å‹ | å¿ƒç‡ | å¤‡æ³¨ |
|---|---|---|---|---|
| 2023-10-01 | 08:00 | 150/95 | 88 | æ™¨èµ·æœªæœè¯ |

## ğŸ§ª å…³é”®æ£€æŸ¥ç»“æœ
...

# æ³¨æ„ï¼š
- åªè¾“å‡º Markdown å†…å®¹ï¼Œä¸è¦åŒ…å« <think> æ ‡ç­¾æˆ–æ— å…³åºŸè¯ã€‚
- ç¡®ä¿æ•°æ®å‡†ç¡®ï¼Œä¸è¦ç¼–é€ ã€‚
`;

const getHeaders = (apiKey: string) => ({ 'Content-Type': 'application/json', 'Authorization': `Bearer ${apiKey}` });
const getBaseUrl = (url?: string) => {
    let base = url ? url.trim() : "https://api.openai.com/v1";
    if (base.endsWith('/')) base = base.slice(0, -1);
    if (!base.endsWith('/v1') && !base.includes('/v1/')) base += '/v1';
    return base;
};

// IMPROVED: Robust JSON cleaner that ignores Markdown blocks and preamble/postscript
const cleanJsonString = (str: string): string => {
    // 1. Locate the first '[' and last ']' to extract the potential array
    const start = str.indexOf('[');
    const end = str.lastIndexOf(']');
    
    if (start !== -1 && end !== -1 && end > start) {
        return str.substring(start, end + 1);
    }
    
    // Fallback: If no array brackets, maybe it wrapped in markdown code block without brackets?
    // Try to remove markdown syntax
    const match = str.match(/```(?:json)?\s*([\s\S]*?)\s*```/i);
    if (match && match[1]) return match[1].trim();

    return str.trim();
};

const sanitizeMessageHistory = (messages: OpenAIMessage[]): OpenAIMessage[] => {
    if (!messages || messages.length === 0) return [];
    const sanitized: OpenAIMessage[] = [];
    const validMessages = [...messages];
    for (let i = 0; i < validMessages.length; i++) {
        const msg = { ...validMessages[i] };
        if (msg.role === 'assistant' && msg.tool_calls && msg.tool_calls.length > 0) {
            const requiredIds = new Set(msg.tool_calls.map(tc => tc.id));
            const foundIds = new Set<string>();
            for (let j = i + 1; j < validMessages.length; j++) {
                const nextMsg = validMessages[j];
                if (nextMsg.role === 'tool') {
                    if (nextMsg.tool_call_id && requiredIds.has(nextMsg.tool_call_id)) foundIds.add(nextMsg.tool_call_id);
                } else break;
            }
            if (requiredIds.size === foundIds.size) sanitized.push(msg);
            else { delete msg.tool_calls; if (msg.content) sanitized.push(msg); }
        } else { if (msg.content || (msg.role === 'assistant' && msg.tool_calls) || msg.role === 'system') sanitized.push(msg); }
    }
    return sanitized;
};

async function fetchWithRetry(url: string, options: RequestInit, retries = MAX_RETRIES, initialDelay = DEFAULT_RETRY_DELAY): Promise<Response> {
  let delay = initialDelay;
  for (let i = 0; i < retries; i++) {
    try {
      const response = await fetch(url, options);
      
      // If success, return immediately
      if (response.ok) return response;

      // Handle 429 (Too Many Requests) and 503 (Service Unavailable) explicitly
      if (response.status === 429 || response.status === 503) {
          const retryAfter = response.headers.get('Retry-After');
          const waitTime = retryAfter ? parseInt(retryAfter) * 1000 : delay;
          console.warn(`[API] Rate limit/Busy (${response.status}). Retrying in ${waitTime}ms... (Attempt ${i + 1}/${retries})`);
          await new Promise(res => setTimeout(res, waitTime));
          delay *= 2; // Exponential backoff
          continue; 
      }

      // Don't retry other client errors (4xx)
      if (response.status >= 400 && response.status < 500) {
          return response;
      }

      // Retry 5xx errors
      if (response.status >= 500) {
           console.warn(`[API] Server error (${response.status}). Retrying... (Attempt ${i + 1}/${retries})`);
           await new Promise(res => setTimeout(res, delay));
           delay *= 2;
           continue;
      }
      
      return response;
    } catch (error: any) {
      // Network errors (fetch failed)
      if (error.name === 'AbortError') throw error;
      
      console.warn(`[API] Network error: ${error.message}. Retrying... (Attempt ${i + 1}/${retries})`);
      if (i === retries - 1) throw error;
      await new Promise(res => setTimeout(res, delay));
      delay *= 2;
    }
  }
  throw new Error(`Request failed after ${retries} retries.`);
}

// ==========================================
// 2. Vector / RAG Functions
// ==========================================

// Supports Single string or Array of strings (Batching)
export const createEmbedding = async (input: string | string[], settings: AISettings): Promise<number[] | number[][] | null> => {
    // IGNORE settings.apiKey/embeddingModel for vectors. Use Built-in.
    // However, we still accept 'settings' argument for interface compatibility.
    const apiKey = VECTOR_API_KEY;
    const baseUrl = VECTOR_API_URL;
    const model = DEFAULT_EMBEDDING_MODEL;
    
    // Safety check for empty input
    if (Array.isArray(input) && input.length === 0) return [];
    if (typeof input === 'string' && !input.trim()) return null;

    // --- CRITICAL FIX FOR 413 ERROR ---
    // SiliconFlow Limit: 8192 tokens. Safe char limit approx 20k.
    const MAX_CHAR_LIMIT = 20000;
    
    const sanitizeInput = (str: string) => {
        if (str.length > MAX_CHAR_LIMIT) {
            console.warn(`[Embedding] Input truncated from ${str.length} to ${MAX_CHAR_LIMIT} chars to avoid 413 error.`);
            return str.slice(0, MAX_CHAR_LIMIT); // Truncate
        }
        return str;
    };

    let processedInput: string | string[];
    
    if (Array.isArray(input)) {
        processedInput = input.map(s => sanitizeInput(s.replace(/\n/g, ' ')));
    } else {
        processedInput = sanitizeInput(input.replace(/\n/g, ' '));
    }

    try {
        const url = `${getBaseUrl(baseUrl)}/embeddings`;
        
        const payload = {
            model: model,
            input: processedInput
        };
        
        // Use default retry mechanism (5 retries with backoff) for embedding
        const res = await fetchWithRetry(url, { 
            method: 'POST', 
            headers: getHeaders(apiKey), 
            body: JSON.stringify(payload) 
        });
        
        if (!res.ok) {
            const errText = await res.text();
            throw new Error(`Embedding failed (SiliconFlow): ${res.status} ${res.statusText} - ${errText.substring(0, 100)}`);
        }
        const data = await res.json();
        
        // Handle response format
        if (data.data && Array.isArray(data.data)) {
            // Sort by index to ensure order matches input
            const sorted = data.data.sort((a: any, b: any) => a.index - b.index);
            
            if (Array.isArray(input)) {
                return sorted.map((d: any) => d.embedding) as number[][];
            } else {
                return sorted[0].embedding as number[];
            }
        }
        return null;
    } catch (e: any) {
        throw e;
    }
};

export const cosineSimilarity = (vecA: number[], vecB: number[]) => {
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    for (let i = 0; i < vecA.length; i++) {
        dotProduct += vecA[i] * vecB[i];
        normA += vecA[i] * vecA[i];
        normB += vecB[i] * vecB[i];
    }
    if (normA === 0 || normB === 0) return 0;
    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
};

export const localVectorSearch = async (
    query: string, 
    chunks: MedicalKnowledgeChunk[], 
    settings: AISettings, 
    topK = 8
): Promise<MedicalKnowledgeChunk[]> => {
    if (chunks.length === 0) return [];
    
    // 1. Always try vector search first since we have built-in engine
    try {
        const queryVec = await createEmbedding(query, settings); // Uses hardcoded engine internally
        if (queryVec && !Array.isArray(queryVec[0])) { // Ensure it's a single vector
            const vec = queryVec as number[];
            const scored = chunks.map(chunk => {
                if (!chunk.embedding) return { chunk, score: -1 };
                return { chunk, score: cosineSimilarity(vec, chunk.embedding) };
            });
            return scored
                .filter(item => item.score > 0.3) // Threshold
                .sort((a, b) => b.score - a.score)
                .slice(0, topK)
                .map(item => item.chunk);
        }
    } catch (e) {
        console.warn("RAG Vector search failed (likely embedding error), falling back to keywords.", e);
    }
    
    // 2. Fallback: Keyword matching
    const keywords = query.split(/[\s,ï¼Œã€‚?!]+/).filter(k => k.length > 1);
    if (keywords.length === 0) return chunks.slice(-topK); // Return latest

    // Simple scoring for keywords
    const scoredChunks = chunks.map(chunk => {
        let score = 0;
        keywords.forEach(k => {
            if (chunk.content.includes(k)) score += 1;
        });
        return { chunk, score };
    });

    return scoredChunks
        .filter(c => c.score > 0)
        .sort((a, b) => b.score - a.score)
        .slice(0, topK)
        .map(item => item.chunk);
};

export const organizeKnowledgeBase = async (chunks: MedicalKnowledgeChunk[], settings: AISettings): Promise<string> => {
    // Uses the passed settings (visitor or admin logic handles key/url)
    if (!settings.apiKey) throw new Error("Missing Chat API Key");
    
    const combinedText = chunks.map(c => c.content).join("\n\n");
    if (combinedText.length > 30000) throw new Error("çŸ¥è¯†åº“å†…å®¹è¿‡é•¿ï¼Œæš‚ä¸æ”¯æŒå…¨é‡æ•´ç†ã€‚"); // Safety cap

    const url = `${getBaseUrl(settings.apiBaseUrl)}/chat/completions`;
    const payload = {
        model: settings.model || "gpt-3.5-turbo",
        messages: [
            { role: "system", content: MEDICAL_ORGANIZE_PROMPT },
            { role: "user", content: `è¯·æ•´ç†ä»¥ä¸‹ç—…å†æ•°æ®ï¼š\n\n${combinedText}` }
        ],
        // DeepSeek models work better with slightly higher temp for creative organization tasks or default
        // But for strict tasks, 0.5 is safer than 0.1 for R1 models to allow 'thinking'
        temperature: 0.6 
    };

    const res = await fetchWithRetry(url, { 
        method: "POST", 
        headers: getHeaders(settings.apiKey), 
        body: JSON.stringify(payload) 
    });
    
    if (!res.ok) throw new Error("Organization failed: " + res.status);
    const data = await res.json();
    let content = data.choices?.[0]?.message?.content || "";
    
    // Remove <think> tags if present (DeepSeek specific)
    content = content.replace(/<think>[\s\S]*?<\/think>/g, "").trim();
    
    return content;
};

// ... (Rest of existing API functions: testModelConnection, fetchAvailableModels, generateHerbDataWithAI etc.)

export const testModelConnection = async (baseUrl: string, apiKey: string): Promise<string> => {
    try {
        const models = await fetchAvailableModels(baseUrl, apiKey);
        return `è¿æ¥æˆåŠŸï¼å…±å‘ç° ${models.length} ä¸ªå¯ç”¨æ¨¡å‹ã€‚`;
    } catch (e: any) { throw new Error(`è¿æ¥å¤±è´¥: ${e.message}`); }
};

export const fetchAvailableModels = async (baseUrl: string, apiKey: string): Promise<ModelOption[]> => {
    try {
        const url = `${getBaseUrl(baseUrl)}/models`;
        const res = await fetchWithRetry(url, { headers: getHeaders(apiKey) });
        if (!res.ok) throw new Error(`Failed to fetch models`);
        const data = await res.json();
        if (data.data && Array.isArray(data.data)) return data.data.map((m: any) => ({ id: m.id, name: m.id }));
        return [];
    } catch (e) { console.error("Model fetch error:", e); throw e; }
};

export const generateHerbDataWithAI = async (herbName: string, settings: AISettings): Promise<BenCaoHerb | null> => {
    if (!settings.apiKey) throw new Error("API Key is missing");
    const systemPrompt = `ä½ æ˜¯ä¸€ä½ç²¾é€šã€Šä¸­åäººæ°‘å…±å’Œå›½è¯å…¸ã€‹(2025ç‰ˆ)çš„ä¸­è¯å­¦ä¸“å®¶ã€‚è¯·è¿”å› ${herbName} çš„ JSON æ•°æ®ã€‚åŒ…å« nature, flavors, meridians, efficacy, usage, processingã€‚`; 
    try {
        const url = `${getBaseUrl(settings.apiBaseUrl)}/chat/completions`;
        const payload = {
            model: settings.model || settings.analysisModel || "gpt-3.5-turbo",
            messages: [{ role: "system", content: systemPrompt }, { role: "user", content: herbName }],
            temperature: 0.1, 
            response_format: { type: "json_object" }
        };
        const res = await fetchWithRetry(url, { method: "POST", headers: getHeaders(settings.apiKey), body: JSON.stringify(payload) });
        if (!res.ok) throw new Error("API call failed");
        const data = await res.json();
        let content = data.choices?.[0]?.message?.content;
        
        // Clean DeepSeek think tags
        if (content) content = content.replace(/<think>[\s\S]*?<\/think>/g, "").trim();

        if (!content) return null;
        const json = JSON.parse(cleanJsonString(content));
        return {
             id: `custom-${Date.now()}`,
             name: json.name || herbName,
             nature: json.nature,
             flavors: json.flavors || [],
             meridians: json.meridians || [],
             efficacy: json.efficacy,
             usage: json.usage,
             category: json.category,
             parentHerb: undefined,
             processing: json.processing,
             isRaw: false
        } as BenCaoHerb;
    } catch (e) { return null; }
};

export async function* analyzePrescriptionWithAI(
    analysis: AnalysisResult,
    prescriptionInput: string,
    settings: AISettings,
    regenerateInstructions?: string,
    existingReport?: string,
    signal?: AbortSignal,
    customSystemInstruction?: string,
    medicalRecord?: MedicalRecord
): AsyncGenerator<string, void, unknown> {
    const url = `${getBaseUrl(settings.apiBaseUrl)}/chat/completions`;
    
    // RAG Retrieval
    let contextStr = "æœªæä¾›è¯¦ç»†ç—…å†ã€‚";
    if (medicalRecord && medicalRecord.knowledgeChunks.length > 0) {
        // Retrieve chunks relevant to the prescription and general analysis keywords
        const query = `${prescriptionInput} ç—…æœº è¯Šæ–­ ç—‡çŠ¶`;
        const relevantChunks = await localVectorSearch(query, medicalRecord.knowledgeChunks, settings, 10);
        
        if (relevantChunks.length > 0) {
            contextStr = relevantChunks.map(c => `- ${c.content}`).join("\n");
        }
    } else {
        // Fallback to structured fields if chunks are empty (Legacy support)
        if (medicalRecord && medicalRecord.basicInfo.name) {
             contextStr = JSON.stringify(medicalRecord, null, 2);
        }
    }

    const context = `ã€å¤„æ–¹åŸæ–‡ã€‘: ${prescriptionInput}\nã€æ‚£è€…ç—…å†çŸ¥è¯†åº“ (RAG Context)ã€‘: \n${contextStr}\n...`; 
    const sysPrompt = customSystemInstruction || settings.systemInstruction || DEFAULT_ANALYZE_SYSTEM_INSTRUCTION;
    const messages: OpenAIMessage[] = [{ role: "system", content: sysPrompt }];
    if (existingReport) {
        messages.push({ role: "user", content: `...` }); 
        messages.push({ role: "assistant", content: existingReport });
        messages.push({ role: "user", content: "Continue..." });
    } else {
        messages.push({ role: "user", content: `è¯·å¯¹ä»¥ä¸‹å¤„æ–¹è¿›è¡Œæ·±åº¦åˆ†æ:\n${context}` });
        if (regenerateInstructions) messages.push({ role: "user", content: `è¡¥å……æŒ‡ä»¤: ${regenerateInstructions}` });
    }
    const payload = {
        model: settings.model || settings.analysisModel || "gpt-3.5-turbo",
        messages: messages,
        temperature: settings.temperature,
        top_p: settings.topP,
        max_tokens: settings.maxTokens || 4000,
        stream: true
    };
    const res = await fetchWithRetry(url, { method: "POST", headers: getHeaders(settings.apiKey), body: JSON.stringify(payload), signal: signal });
    if (!res.ok) throw new Error(`AI Analysis Failed`);
    if (!res.body) return;
    const reader = res.body.getReader();
    const decoder = new TextDecoder("utf-8");
    let buffer = "";
    try {
        while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split("\n");
            buffer = lines.pop() || "";
            for (const line of lines) {
                if (line.trim().startsWith("data: ")) {
                    const dataStr = line.slice(6).trim();
                    if (dataStr === "[DONE]") return;
                    try {
                        const json = JSON.parse(dataStr);
                        const chunk = json.choices[0]?.delta?.content;
                        if (chunk) {
                            // DeepSeek: Skip <think> content if user wants raw output, but streaming is tricky. 
                            // For report generation, we often want just the result. 
                            // However, filtering <think> in stream is hard. We assume user accepts think trace or model obeys system prompt.
                            yield chunk;
                        }
                    } catch (e) {}
                }
            }
        }
    } finally { reader.releaseLock(); }
};

export async function* generateChatStream(
    history: any[], 
    analysis: AnalysisResult,
    prescription: string,
    reportContent: string | undefined,
    settings: AISettings,
    signal: AbortSignal | undefined,
    medicalRecord: MedicalRecord,
    systemInstruction: string 
): AsyncGenerator<{ text?: string, functionCalls?: {id: string, name: string, args: any}[] }, void, unknown> {
    const url = `${getBaseUrl(settings.apiBaseUrl)}/chat/completions`;
    
    // Perform RAG for the latest user message
    let ragContext = "";
    const lastUserMsg = history.filter(m => m.role === 'user').pop();
    if (lastUserMsg && medicalRecord.knowledgeChunks.length > 0) {
        const chunks = await localVectorSearch(lastUserMsg.text, medicalRecord.knowledgeChunks, settings, 5);
        if (chunks.length > 0) {
            // INCLUDE CHUNK IDs in Context so LLM can reference them for updates
            ragContext = `\n\n**ç›¸å…³ç—…å†çŸ¥è¯† (Retrieval Context)**:\n${chunks.map(c => `> [ID: ${c.id}] ${c.content}`).join('\n')}`;
        }
    }

    const systemMsg: OpenAIMessage = { role: "system", content: systemInstruction + ragContext };
    
    const apiHistory: OpenAIMessage[] = history.map(m => {
        if (m.role === 'system') return { role: 'system', content: m.text };
        if (m.role === 'tool') return { role: 'tool', content: m.text, tool_call_id: m.toolCallId };
        const role = m.role === 'model' ? 'assistant' : 'user';
        return { role, content: m.text, tool_calls: m.toolCalls };
    }); 
    
    const payload = {
        model: settings.model || settings.chatModel || "gpt-3.5-turbo",
        messages: sanitizeMessageHistory([systemMsg, ...apiHistory]),
        temperature: 0.5, 
        stream: true,
        tool_choice: "auto", 
        tools: [
            { type: "function", function: { name: "lookup_herb", parameters: { type: "object", properties: { query: { type: "string" } }, required: ["query"] } } },
            { type: "function", function: { name: "update_prescription", parameters: { type: "object", properties: { prescription: { type: "string" } }, required: ["prescription"] } } },
            { type: "function", function: { name: "regenerate_report", parameters: { type: "object", properties: { instructions: { type: "string" } }, required: ["instructions"] } } },
            { 
                type: "function", 
                function: { 
                    name: "save_medical_info", 
                    description: "Save NEW medical information (append) or key insights found in conversation.",
                    parameters: { 
                        type: "object", 
                        properties: { 
                            category: { type: "string", description: "Category like 'è¡€å‹', 'ä¸»è¯‰', 'ç”¨è¯åé¦ˆ'" },
                            content: { type: "string", description: "The content to save." } 
                        }, 
                        required: ["category", "content"] 
                    } 
                } 
            },
            // NEW TOOL: Update Existing Chunk
            {
                type: "function", 
                function: {
                    name: "update_knowledge_chunk",
                    description: "Modify an existing knowledge chunk to fix errors (e.g. OCR typos) or update status.",
                    parameters: {
                        type: "object",
                        properties: {
                            chunkId: { type: "string", description: "The ID of the chunk to update." },
                            newContent: { type: "string", description: "The corrected or updated content." }
                        },
                        required: ["chunkId", "newContent"]
                    }
                }
            },
            // GOD MODE TOOLS
            {
                type: "function",
                function: {
                    name: "update_herb_database",
                    description: "Modify or Add a herb entry in the global database (è¯æåº“). Use this to fix wrong nature/flavor or add new herbs.",
                    parameters: {
                        type: "object",
                        properties: {
                            name: { type: "string", description: "Herb name" },
                            nature: { type: "string", description: "Nature (e.g., æ¸©, å¯’)" },
                            flavors: { type: "array", items: { type: "string" }, description: "Flavors (e.g., ['è¾›', 'ç”˜'])" },
                            meridians: { type: "array", items: { type: "string" }, description: "Meridians (e.g., ['è‚º', 'è„¾'])" },
                            efficacy: { type: "string", description: "Efficacy description" },
                            usage: { type: "string", description: "Usage instructions" }
                        },
                        required: ["name"]
                    }
                }
            },
            {
                type: "function",
                function: {
                    name: "update_medical_record_full",
                    description: "Update basic info or structured fields of the medical record (e.g. Name, Age, Diagnosis). NOT for appending text chunks.",
                    parameters: {
                        type: "object",
                        properties: {
                            name: { type: "string" },
                            age: { type: "string" },
                            gender: { type: "string" },
                            tcmDiagnosis: { type: "string", description: "TCM Diagnosis" }
                        }
                    }
                }
            }
        ]
    };
    const res = await fetchWithRetry(url, { method: "POST", headers: getHeaders(settings.apiKey), body: JSON.stringify(payload), signal: signal });
    if (!res.body) return;
    const reader = res.body.getReader();
    const decoder = new TextDecoder("utf-8");
    let buffer = "";
    let currentToolCalls: any = {};
    
    let hasOutputThinking = false;

    try {
        while(true) {
            const { done, value } = await reader.read();
            if (done) break;
            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split("\n");
            buffer = lines.pop() || "";
            for (const line of lines) {
                if (line.trim().startsWith("data: ")) {
                    const dataStr = line.slice(6).trim();
                    if (dataStr === "[DONE]") continue;
                    try {
                        const json = JSON.parse(dataStr);
                        const delta = json.choices[0].delta;
                        
                        // --- FIX FOR EMPTY REPLIES (DeepSeek R1 / SiliconFlow) ---
                        // Capture 'reasoning_content' which is often sent before 'content'.
                        // We format it as a blockquote or pass it raw so the UI can render it.
                        // Standardizing it to Markdown Quote for compatibility.
                        if (delta.reasoning_content) {
                            if (!hasOutputThinking) {
                                yield { text: "> **Thinking Process:**\n> " };
                                hasOutputThinking = true;
                            }
                            // Prepend '> ' to new lines to keep blockquote format, but simple stream append works too if UI parses markdown line by line
                            // For simplicity, we just yield the text. Ideally user interface handles <think> tags, but R1 API uses a separate field.
                            // We stream it as text so it's visible.
                            const formattedThinking = delta.reasoning_content.replace(/\n/g, "\n> ");
                            yield { text: formattedThinking };
                        }

                        // Standard Content
                        if (delta.content) {
                            if (hasOutputThinking) {
                                // Add a break after thinking finishes if we just switched
                                yield { text: "\n\n" };
                                hasOutputThinking = false;
                            }
                            yield { text: delta.content };
                        }
                        
                        if (delta.tool_calls) {
                            delta.tool_calls.forEach((toolDelta: any) => {
                                const index = toolDelta.index;
                                if (!currentToolCalls[index]) currentToolCalls[index] = { id: '', name: '', args: '' };
                                if (toolDelta.id) currentToolCalls[index].id = toolDelta.id;
                                if (toolDelta.function?.name) currentToolCalls[index].name = toolDelta.function.name;
                                if (toolDelta.function?.arguments) currentToolCalls[index].args += toolDelta.function.arguments;
                            });
                        }
                    } catch (e) {}
                }
            }
        }
        const parsedCalls = Object.values(currentToolCalls).map((tc: any) => {
            try { return { id: tc.id, name: tc.name, args: JSON.parse(tc.args) }; } catch(e){ return null; }
        }).filter(c => c!==null);
        if (parsedCalls.length > 0) yield { functionCalls: parsedCalls as any };
    } finally { reader.releaseLock(); }
}

export const summarizeMessages = async (messages: any[], settings: AISettings): Promise<string> => {
    if (!settings.apiKey) return "Error: API Key missing.";
    const contentToSummarize = messages.map(m => `${m.role}: ${JSON.stringify(m.text || m.content)}`).join("\n");
    const systemPrompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—ä¹¦è®°å‘˜ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²æ€»ç»“ä¸ºä¸€ä»½ç®€æ´çš„ã€æŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„åŒ»ç–—æ‘˜è¦ã€‚æ¶µç›–å…³é”®ç—‡çŠ¶ã€è¯Šæ–­ã€æ²»ç–—å’Œæ‚£è€…é—®é¢˜ã€‚ç®€æ˜æ‰¼è¦ï¼Œå®äº‹æ±‚æ˜¯ã€‚";
    try {
        const url = `${getBaseUrl(settings.apiBaseUrl)}/chat/completions`;
        const payload = {
            model: settings.model || "gpt-3.5-turbo",
            messages: [{ role: "system", content: systemPrompt }, { role: "user", content: contentToSummarize }],
            temperature: 0.3
        };
        const res = await fetchWithRetry(url, { method: "POST", headers: getHeaders(settings.apiKey), body: JSON.stringify(payload) });
        if (!res.ok) {
            const errorBody = await res.text();
            console.error("Summary failed with status:", res.status, "body:", errorBody);
            throw new Error(`Summary API call failed: ${res.status}`);
        }
        const data = await res.json();
        let content = data.choices?.[0]?.message?.content || "Summary generation failed.";
        content = content.replace(/<think>[\s\S]*?<\/think>/g, "").trim();
        return content;
    } catch (e: any) { 
        console.error("Error in summarizeMessages:", e);
        return `Summary failed: ${e.message}`; 
    }
};